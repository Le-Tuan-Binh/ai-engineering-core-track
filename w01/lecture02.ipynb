{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Chat Completions API\n",
    "\n",
    "The simplest and most common way to call a Large Language Model (LLM) is through the **Chat Completions API**. The idea behind this interface is straightforward you provide a sequence of messages that represent a conversation, and the model predicts what should come next. In other words, it completes the chat. This format is intuitive for both humans and machines, which is why it has become the dominant method for interacting with LLMs.\n",
    "\n",
    "Although the Chat Completions API was originally introduced by OpenAI, it quickly became a standard across the industry. Most modern AI platforms open-source and commercial now support the same message-based interface because it provides a consistent, structured way to guide model behavior, maintain context, and control responses.\n",
    "\n",
    "At its core, the API takes an ordered list of messages. Each message includes a role **system**, **user**, or **assistant** and content which is the text being communicated. The **system role** defines the **model’s behavior**, the **user role** contains the **actual request**, and the **assistant role** represents the **model’s previous replies**. By structuring input this way, applications can build rich, multi-turn interactions that feel natural and maintain state across calls.\n",
    "\n",
    "This design makes **Chat Completions** ideal for **chatbots**, **automated customer support**, **interactive agents**, **tutoring systems**, and any application where a conversational experience is needed. It also serves as an accessible entry point for beginners, requiring no complex setup—only a list of messages and a single API call."
   ],
   "id": "d18c7f28ad97e321"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Get the OpenAI Key\n",
    "\n",
    "This section loads environment variables from the `.env` file and connects to OpenAI using the `API_KEY`.\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "1. Create a `.env` file in your project directory.\n",
    "\n",
    "2. Add the following line to the file, replacing the value with your actual OpenAI API key `OPENAI_API_KEY=sk-xxxxxxx`\n",
    "\n",
    "3. Run the script below to load and verify the API key."
   ],
   "id": "f19e5307538fe7fd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(override=True)\n",
    "api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "def validate_api_key(key: str):\n",
    "    if not key:\n",
    "        raise ValueError(\n",
    "            \"❌ No API key found! Please check the troubleshooting notebook.\"\n",
    "        )\n",
    "    if not key.startswith(\"sk-proj-\"):\n",
    "        raise ValueError(\n",
    "            \"⚠️ API key found, but it doesn't start with 'sk-proj-'. Please use the correct key.\"\n",
    "        )\n",
    "    if key.strip() != key:\n",
    "        raise ValueError(\n",
    "            \"⚠️ API key has leading or trailing whitespace. Please remove them.\"\n",
    "        )\n",
    "    print(\"✔ API key has found and looks good!\")\n",
    "\n",
    "validate_api_key(api_key)"
   ],
   "id": "221536ef421c68e2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Using an endpoint to call the Chat Completions API\n",
    "\n",
    "An endpoint is the url exposed by an API where you send a request and receive a response. With OpenAI's Chat Completions API, the endpoint might look like\n",
    "\n",
    "```bash\n",
    "POST https://api.openai.com/v1/chat/completions\n",
    "```\n",
    "\n",
    "When calling an LLM, you send a **POST request** with some information:\n",
    "\n",
    "- Headers, for authorization and content type\n",
    "\n",
    "- Payload, include messages, model, and other parameters"
   ],
   "id": "da8e280206018fd8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import requests\n",
    "\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {api_key}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "payload = {\n",
    "    \"model\": \"gpt-5-nano\",\n",
    "    \"messages\": [\n",
    "        {\"role\": \"user\", \"content\": \"Tell me a fun fact\"}\n",
    "    ]\n",
    "}\n",
    "\n",
    "payload"
   ],
   "id": "92c19eeceb427ae2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "response = requests.post(\n",
    "    \"https://api.openai.com/v1/chat/completions\",\n",
    "    headers=headers,\n",
    "    json=payload\n",
    ")\n",
    "\n",
    "response.json()"
   ],
   "id": "a49ec678929d7aa4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "response.json()[\"choices\"][0][\"message\"][\"content\"]",
   "id": "bc7941c538ca86b6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# OpenAI Package\n",
    "\n",
    "Using HTTP endpoints can be **cumbersome** that you need to construct **HTTP requests**, set headers correctly, **handle authentication tokens**, and format **JSON payloads** precisely. On top of that, you must handle network errors, rate limits, and parse responses manually, which can make the code verbose and error-prone.\n",
    "\n",
    "The OpenAI Python package simplifies this process by providing a cleaner syntax that **abstracts** these details. It includes **built-in error handling**, **raises informative exceptions for common issues**, and **offers helpful parameter defaults** and **utility functions**, making it **easier to manage models**, **responses**, and advanced features like **streaming**."
   ],
   "id": "d78bdd3b064b8812"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## ChatGPT",
   "id": "9e28d09f43e8fc7c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Create OpenAI Client",
   "id": "b90841ce1cbc2624"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "openai = OpenAI()"
   ],
   "id": "de3b4e3f06ac6394",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Let's ask for a fun fact",
   "id": "a0f58ad9955cb472"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "response = openai.chat.completions.create(\n",
    "    model=\"gpt-5-nano\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Tell me a fun fact\"\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "response.choices[0].message.content"
   ],
   "id": "ff4186e3da77440f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## And then this great thing happened\n",
    "\n",
    "**OpenAI's Chat Completions API** became so popular that other model providers started offering OpenAI-compatible endpoints.\n",
    "\n",
    "OpenAI made it even easier so you can use the same Python client library developed for GPT, simply by specifying a different endpoint URL and API key to access another provider.\n",
    "\n",
    "So you can use\n",
    "\n",
    "```python\n",
    "from openai import OpenAI\n",
    "\n",
    "gemini = OpenAI(\n",
    "    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\",\n",
    "    api_key=\"AIz....\"\n",
    ")\n",
    "\n",
    "gemini.chat.completions.create(\n",
    "    model=\"gemini-1.5\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Tell me a fun fact\"\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "```\n",
    "\n",
    "Note that even though references the OpenAI Python client, it is only acting as a **generic client** to call the endpoint you specify. It means no OpenAI models are being used the actual computation is handled entirely by the provider at the endpoint."
   ],
   "id": "df95d9aef9fa47e0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Google Gemini",
   "id": "1beb2d669bd5c731"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Create a Google Gemini API Key\n",
    "\n",
    "**1. Go to AI Studio**\n",
    "\n",
    "Visit https://aistudio.google.com/, use your Google account to sign in. If you don’t have an account, create one.\n",
    "\n",
    "**2. Navigate to API Keys**\n",
    "\n",
    "Once signed in, go to the API keys https://aistudio.google.com/api-keys\n",
    "\n",
    "**3. Create a new api key**\n",
    "\n",
    "- Click on the button to create a new key.\n",
    "\n",
    "- Give your key a name, for example `defautl`.\n",
    "\n",
    "- Select the permissions or roles required for your use case, usually “Read & Write” for API access.\n",
    "\n",
    "**4. Storage Your API Key**\n",
    "\n",
    "After creation, the API key will be displayed, storage it immediately, some platforms only show it once."
   ],
   "id": "619620438e14772b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Add the key into the `.env` file\n",
    "\n",
    "Open your `.env` file in your project, add the line like\n",
    "\n",
    "```bash\n",
    "GOOGLE_API_KEY=AIz...\n",
    "```\n",
    "\n",
    "Replace `AIz...` with your actual API key then save the .env file."
   ],
   "id": "574c6f158195a9a8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Get the Gemini Key",
   "id": "404fdd5a8870eb57"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Gemini endpoint\n",
    "GEMINI_BASE_URL = \"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
    "\n",
    "# Load .env file\n",
    "load_dotenv(override=True)\n",
    "\n",
    "# Get the Gemini API key\n",
    "google_api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "\n",
    "def validate_gemini_api_key(key: str):\n",
    "    if not key:\n",
    "        raise ValueError(\n",
    "            \"❌ No Gemini API key found! Please add your key to the .env file and save it.\"\n",
    "        )\n",
    "    if not key.startswith(\"AIz\"):\n",
    "        raise ValueError(\n",
    "            \"⚠️ API key found, but it doesn't start with 'AIz'. Please use the correct Gemini key.\"\n",
    "        )\n",
    "    if key.strip() != key:\n",
    "        raise ValueError(\n",
    "            \"⚠️ API key has leading or trailing whitespace. Please remove them.\"\n",
    "        )\n",
    "    print(\"✔ Gemini API key found and looks good!\")\n",
    "\n",
    "# Validate the key\n",
    "validate_gemini_api_key(google_api_key)\n"
   ],
   "id": "d1dda41b2a6123c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Create the OpenAI Client",
   "id": "1044245cb4035b59"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "gemini = OpenAI(base_url=GEMINI_BASE_URL, api_key=google_api_key)"
   ],
   "id": "a173270717efca9e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Let's ask for a fun fact",
   "id": "ec8cd94f0e97007d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "response = gemini.chat.completions.create(\n",
    "    model=\"gemini-2.5-flash-lite\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Tell me a fun fact\"\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "response.choices[0].message.content"
   ],
   "id": "bd57e5df0e2596c3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Ollama",
   "id": "cc70c360baa0e0a3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Compatible model\n",
    "\n",
    "Ollama uses LLaMA models from Meta.\n",
    "\n",
    "```bash\n",
    "llama3.2\n",
    "```\n",
    "\n",
    "If your computer has limited resources, use the smaller version\n",
    "\n",
    "```bash\n",
    "llama3.2:1b\n",
    "```\n",
    "\n",
    "Not using llama3.3 or llama4 because they are too large for most personal computers.\n",
    "\n",
    "Try to download the `llama3.2` with script below\n",
    "\n",
    "```bash\n",
    "ollama pull llama3.2\n",
    "```"
   ],
   "id": "f586c9535180681d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Running the Ollama server\n",
    "\n",
    "Try to excute the following, if it prints \"Ollama is running\", the server is active and ready to accept requests. If not, you need to start the Ollama server manually.\n",
    "\n",
    "Open the terminal and run\n",
    "```bash\n",
    "ollama serve\n",
    "```\n",
    "\n",
    "It will start the local server at http://localhost:11434."
   ],
   "id": "4381f1a78fedf2ad"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import requests\n",
    "print(requests.get(\"http://localhost:11434\").content)"
   ],
   "id": "8c016b7853d10070",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Create the OpenAI Client",
   "id": "edb1bd1534e3e592"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "OLLAMA_BASE_URL = \"http://localhost:11434/v1\"\n",
    "\n",
    "ollama = OpenAI(base_url=OLLAMA_BASE_URL, api_key='ollama')"
   ],
   "id": "3c49c95f4f723791",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Note that the `/v1` in the base url is required because the OpenAI-compatible client expects API version 1 endpoints. Without `/v1`, requests to the server will fail.\n",
   "id": "1ea63068be11ee8e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Let's ask for a fun fact",
   "id": "391204e45ead39c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "response = ollama.chat.completions.create(\n",
    "    model=\"llama3.2\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Tell me a fun fact\"\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "response.choices[0].message.content"
   ],
   "id": "29413b17efe8d6c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We can use with another models support by Ollama like `deepseek-r1:1.5b`, which is \"distilled\" into Qwen from Alibaba Cloud",
   "id": "86f705942bbad288"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "!ollama pull deepseek-r1:1.5b",
   "id": "d46afe2eb83069ba",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "response = ollama.chat.completions.create(\n",
    "    model=\"deepseek-r1:1.5b\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Tell me a fun fact\"\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "response.choices[0].message.content"
   ],
   "id": "71edc56ca2656142",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
